{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40eed33b-9603-430b-9300-f2f1a7df6d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: xgboost in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f0159fa3-801a-4dab-b0f3-f658d7049045/lib/python3.9/site-packages (2.1.4)\nRequirement already satisfied: nvidia-nccl-cu12 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f0159fa3-801a-4dab-b0f3-f658d7049045/lib/python3.9/site-packages (from xgboost) (2.26.2.post1)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.7.3)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.21.5)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25dc065b-115e-4728-bbfb-aabd1a250e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum as Fsum, isnan, when, create_map, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from xgboost.spark import SparkXGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465e50a1-77f6-43e8-ac6a-1dc5d5f22020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"PodcastListeningTimePrediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d034ab3c-852b-412e-bf8f-43b0fc2eea8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"\"\n",
    "SECRET_KEY = \"\"\n",
    "bucket_name = \"podcasts5e4\"\n",
    "\n",
    "# Configure Spark for S3 access\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "# S3 paths\n",
    "train_path = f\"s3a://{bucket_name}/train.csv\"\n",
    "\n",
    "# Load CSV files\n",
    "train = spark.read.csv(train_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b55da9a-5989-4a01-84fa-63787d66ea4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.na.drop(subset=[\"Listening_Time_minutes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a9dcd3-e80e-4141-bde4-32d3e9ae6fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate medians by Genre\n",
    "genre_median = train.groupBy(\"Genre\").agg(\n",
    "    F.expr(\"percentile_approx(Episode_Length_minutes, 0.5)\").alias(\"Genre_Median\")\n",
    ")\n",
    "\n",
    "# Join and impute in train\n",
    "train = train.join(genre_median, on=\"Genre\", how=\"left\")\n",
    "train = train.withColumn(\n",
    "    \"Episode_Length_minutes\",\n",
    "    F.when(\n",
    "        F.col(\"Episode_Length_minutes\").isNull(), F.col(\"Genre_Median\")\n",
    "    ).otherwise(F.col(\"Episode_Length_minutes\"))\n",
    ").drop(\"Genre_Median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3c2004-8ee9-46e1-9cba-db951a70d8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.withColumn(\n",
    "    \"Guest_Present\", F.when(F.col(\"Guest_Popularity_percentage\").isNotNull(), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"Guest_Popularity_percentage\", F.coalesce(\"Guest_Popularity_percentage\", F.lit(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1986d3-6907-4432-88d3-500d715e2466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ads_mode_row = train.groupBy(\"Number_of_Ads\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .first()\n",
    "ads_mode = ads_mode_row[\"Number_of_Ads\"]\n",
    "\n",
    "train = train.withColumn(\n",
    "    \"Number_of_Ads\",\n",
    "    F.when(F.col(\"Number_of_Ads\").isNull(), ads_mode).otherwise(F.col(\"Number_of_Ads\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c03fbb8-1f36-4309-9ae0-480d143398b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMissing value check for Train:\n  Genre: âœ… no missing values\n  id: âœ… no missing values\n  Podcast_Name: âœ… no missing values\n  Episode_Title: âœ… no missing values\n  Episode_Length_minutes: âœ… no missing values\n  Host_Popularity_percentage: âœ… no missing values\n  Publication_Day: âœ… no missing values\n  Publication_Time: âœ… no missing values\n  Guest_Popularity_percentage: âœ… no missing values\n  Number_of_Ads: âœ… no missing values\n  Episode_Sentiment: âœ… no missing values\n  Listening_Time_minutes: âœ… no missing values\n  Guest_Present: âœ… no missing values\n"
     ]
    }
   ],
   "source": [
    "def print_missing_values(df, df_name=\"DataFrame\"):\n",
    "    print(f\"\\nMissing value check for {df_name}:\")\n",
    "    total_rows = df.count()\n",
    "    for column in df.columns:\n",
    "        missing = df.select(\n",
    "            Fsum(when(col(column).isNull() | isnan(column), 1).otherwise(0)).alias(\"missing\")\n",
    "        ).collect()[0][\"missing\"]\n",
    "        if missing > 0:\n",
    "            print(f\"  {column}: {missing} missing ({missing / total_rows:.2%})\")\n",
    "        else:\n",
    "            print(f\"  {column}: âœ… no missing values\")\n",
    "\n",
    "print_missing_values(train, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843f6d5d-27d2-406a-bc9b-eb462c951914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n|   Podcast_Name|   Podcast_Name_Freq|\n+---------------+--------------------+\n|    Mind & Body|0.018185333333333335|\n| Digital Digest| 0.02156133333333333|\n|  Joke Junction|0.020098666666666667|\n|  Fitness First|            0.025984|\n|Mystery Matters|            0.021336|\n+---------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Count frequencies and normalize\n",
    "podcast_freq = train.groupBy(\"Podcast_Name\").count()\n",
    "total_train = train.count()\n",
    "podcast_freq = podcast_freq.withColumn(\"Podcast_Name_Freq\", F.col(\"count\") / total_train).drop(\"count\")\n",
    "\n",
    "# Join back to get frequency feature\n",
    "train = train.join(podcast_freq, on=\"Podcast_Name\", how=\"left\")\n",
    "# Sanity check\n",
    "train.select(\"Podcast_Name\", \"Podcast_Name_Freq\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a645200b-adfc-4a97-937e-6a061efce2b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n|Episode_Title|Episode_Number|\n+-------------+--------------+\n|   Episode 98|          98.0|\n|   Episode 26|          26.0|\n|   Episode 16|          16.0|\n|   Episode 45|          45.0|\n|   Episode 86|          86.0|\n+-------------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Extract digits\n",
    "train = train.withColumn(\"Episode_Number\", F.regexp_extract(\"Episode_Title\", r\"(\\d+)\", 1).cast(\"float\"))\n",
    "\n",
    "# Fill missing with median from train\n",
    "episode_number_median = train.approxQuantile(\"Episode_Number\", [0.5], 0.01)[0]\n",
    "\n",
    "train = train.withColumn(\n",
    "    \"Episode_Number\", F.when(F.col(\"Episode_Number\").isNull(), episode_number_median).otherwise(F.col(\"Episode_Number\"))\n",
    ")\n",
    "# Sanity check\n",
    "train.select(\"Episode_Title\", \"Episode_Number\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f47a913-93d3-4951-9cf6-d8a2bd8f81fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+-----------------+\n|Publication_Day|Publication_Time|Episode_Sentiment|\n+---------------+----------------+-----------------+\n|              4|               4|                2|\n|              6|               2|                0|\n|              2|               3|                0|\n|              1|               1|                2|\n|              1|               2|                1|\n+---------------+----------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Create mapping dictionaries\n",
    "day_map = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4,\n",
    "           'Friday': 5, 'Saturday': 6, 'Sunday': 7}\n",
    "time_map = {'Morning': 1, 'Afternoon': 2, 'Evening': 3, 'Night': 4}\n",
    "sentiment_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "\n",
    "def map_column(df, colname, mapping):\n",
    "    mapping_expr = create_map(*[lit(x) for kv in mapping.items() for x in kv])\n",
    "    return df.withColumn(colname, mapping_expr[col(colname)])\n",
    "\n",
    "train = map_column(train, \"Publication_Day\", day_map)\n",
    "\n",
    "train = map_column(train, \"Publication_Time\", time_map)\n",
    "\n",
    "train = map_column(train, \"Episode_Sentiment\", sentiment_map)\n",
    "\n",
    "# Sanity check\n",
    "train.select(\"Publication_Day\", \"Publication_Time\", \"Episode_Sentiment\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670a4111-a41b-4d2c-9a40-44dff1b43177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"Podcast_Name\", \"Episode_Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b452842-ee10-414e-8be4-93c9ba50a2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"Genre\", outputCol=\"Genre_Idx\")\n",
    "genre_indexer_model = indexer.fit(train)\n",
    "\n",
    "train = genre_indexer_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd584274-a26d-47c2-a316-6dfdcb9bcb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"Genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8df34cf-0efa-4ecf-b2b5-d1ccc804b893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|Genre_Idx|count|\n+---------+-----+\n|      8.0|62743|\n|      0.0|87606|\n|      7.0|63385|\n|      1.0|86256|\n|      4.0|81453|\n|      3.0|82461|\n|      2.0|85059|\n|      6.0|71416|\n|      5.0|80521|\n|      9.0|49100|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "train.groupBy('Genre_Idx').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf45ab61-ea43-4dbf-83f6-5ef1cf7856c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_split, val_split = train.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define feature columns by excluding the target\n",
    "feature_cols = [col for col in train.columns if col != \"Listening_Time_minutes\"]\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1265908-1cd6-4386-9e9b-7be5b7b42c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 14:04:18,752 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:04:18,810 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:05:31,319 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:05:33,452 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:06:53,838 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:07:26,145 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:07:57,390 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:08:00,086 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:08:43,123 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:09:16,621 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:09:45,940 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:09:50,057 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:10:33,137 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:11:07,648 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:11:35,858 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:11:39,194 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:12:44,170 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:12:44,237 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:13:49,248 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:13:51,726 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:15:04,956 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:15:34,643 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:16:02,621 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:16:05,097 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:16:47,597 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:17:17,348 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:17:48,864 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:17:51,943 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:18:35,267 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:19:04,793 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:19:33,384 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:19:35,982 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:20:41,249 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:20:41,274 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:21:48,373 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:21:50,555 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:23:04,446 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:23:35,470 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 4, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:24:02,274 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:24:05,523 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:24:48,035 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:25:17,748 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:25:45,864 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:25:49,203 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:26:29,623 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:26:59,543 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:27:29,356 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:27:33,068 INFO XGBoost-PySpark: _fit Finished xgboost training!\n2025-04-24 14:28:46,785 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 2 workers with\n\tbooster params: {'device': 'cpu', 'learning_rate': 0.2, 'max_depth': 6, 'objective': 'reg:squarederror', 'nthread': 1}\n\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n2025-04-24 14:29:18,685 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Regressor\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"Listening_Time_minutes\",\n",
    "    prediction_col=\"prediction\",\n",
    "    objective='reg:squarederror',\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, xgb])\n",
    "\n",
    "# Cross-validator\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb.max_depth, [4, 6]) \\\n",
    "    .addGrid(xgb.learning_rate, [0.1, 0.2]) \\\n",
    "    .addGrid(xgb.n_estimators, [50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Listening_Time_minutes\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "cv_model = cv.fit(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05790c66-4bd6-43fe-b343-cdd7a61edaa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Validation RMSE: 13.0539\n"
     ]
    }
   ],
   "source": [
    "val_predictions = cv_model.transform(val_split)\n",
    "val_rmse = evaluator.evaluate(val_predictions)\n",
    "print(f\"ðŸ“Š Validation RMSE: {val_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark avg Listening Time",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}